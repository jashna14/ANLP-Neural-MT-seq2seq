{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(): \n",
    "    \n",
    "    fe = open('english_sampled.txt',encoding='utf-8')\n",
    "    Linese = fe.readlines()\n",
    "    \n",
    "    fh = open('hindi_sampled.txt',encoding='utf-8')\n",
    "    Linesh = fh.readlines()\n",
    "    \n",
    "    en_sentences = list()\n",
    "    maxen_seqlen = 0\n",
    "    for line in Linese:\n",
    "        line = re.sub('[()]', '', line)\n",
    "        line = re.sub(r\"([?.!,¿;।])\", r\" \\1 \", line)\n",
    "        line = re.sub(r'[\" \"]+', \" \", line)\n",
    "        line = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", line)\n",
    "        line = line.lower()\n",
    "        line = line.strip()\n",
    "        line = line.split(' ')\n",
    "        line = line[::-1]\n",
    "        line.insert(0,'<sos>')\n",
    "        line.append('<eos>')\n",
    "        maxen_seqlen = max(len(line),maxen_seqlen)\n",
    "        en_sentences.append(line)\n",
    "\n",
    "    hi_sentences = list()\n",
    "    maxhi_seqlen = 0\n",
    "    for line in Linesh:\n",
    "        line = re.sub('[()]', '', line)\n",
    "        line = re.sub(r\"([?.!,¿;।])\", r\" \\1 \", line)\n",
    "        line = re.sub(r'[\" \"]+', \" \", line)\n",
    "        line = line.strip()\n",
    "       \n",
    "        if(line[0] == '-'):\n",
    "            line = line[1:]\n",
    "        elif(line[0] == '-' and line[1] == ' '):\n",
    "            line = line[2:]\n",
    "        \n",
    "        line = line.strip()\n",
    "        line = '<sos> ' + line + ' <eos>'\n",
    "        \n",
    "        line = line.split(' ')\n",
    "        maxhi_seqlen = max(maxhi_seqlen , len(line))\n",
    "            \n",
    "        hi_sentences.append(line)\n",
    "\n",
    "    return en_sentences , hi_sentences , maxen_seqlen , maxhi_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sen, trg_sen, maxsrc_seqlen, maxtrg_seqlen =  extract_data()\n",
    "print(maxsrc_seqlen, maxtrg_seqlen)\n",
    "\n",
    "\n",
    "\n",
    "src_train, src_test, trg_train, trg_test = train_test_split(src_sen, trg_sen, test_size=0.30)\n",
    "src_dev, src_test, trg_dev, trg_test = train_test_split(src_test, trg_test, test_size=2/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, min_freq):\n",
    "    vocab = dict()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 0\n",
    "            vocab[word] += 1\n",
    "    \n",
    "    keys = list(vocab.keys())\n",
    "    \n",
    "    new_vocab = list()\n",
    "    \n",
    "    for key in keys:\n",
    "        if(vocab[key] > min_freq):\n",
    "            new_vocab.append(key)\n",
    "    \n",
    "    pad = \"<pad>\"\n",
    "    sos = \"<sos>\"\n",
    "    eos = \"<eos>\"\n",
    "    unk = \"<unk>\"\n",
    "\n",
    "    \n",
    "    id2word = {}\n",
    "    word2id = {}\n",
    "    \n",
    "    id2word[0] = pad \n",
    "    id2word[1] = sos \n",
    "    id2word[2] = eos\n",
    "    id2word[3] = unk\n",
    "    \n",
    "    word2id[pad] = 0 \n",
    "    word2id[sos] = 1\n",
    "    word2id[eos] = 2\n",
    "    word2id[unk] = 3\n",
    "    \n",
    "    cur_id = 4\n",
    "    \n",
    "    for word in new_vocab:\n",
    "        if word not in word2id:\n",
    "            word2id[word] = cur_id\n",
    "            id2word[cur_id] = word\n",
    "            cur_id += 1\n",
    "\n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences, max_len, word2id):\n",
    "    encoded_sentences = list()\n",
    "    \n",
    "    for sentence in sentences: \n",
    "        enc_sen = []\n",
    "        for word in sentence:\n",
    "            if word in word2id:\n",
    "                enc_sen.append(word2id[word])\n",
    "            else:\n",
    "                ''' appending index corresponding to <unk> '''\n",
    "                enc_sen.append(3)\n",
    "        \n",
    "        for i in range(max_len - len(enc_sen)):\n",
    "            ''' appending index corresponding to <pad> for padding '''\n",
    "            enc_sen.append(0)\n",
    "        \n",
    "        encoded_sentences.append(enc_sen)\n",
    "    \n",
    "    return encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sentences(sentence, id2word):\n",
    "    decoded_sentence = list() \n",
    "    for i in sentence:\n",
    "        if(i not in [0,1,2]):\n",
    "            decoded_sentence.append(id2word[i])\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_word2id, src_id2word = build_vocab(src_train, 1)\n",
    "trg_word2id, trg_id2word = build_vocab(trg_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_src_train = encode_sentences(src_train, maxsrc_seqlen, src_word2id)\n",
    "enc_src_test = encode_sentences(src_test, maxsrc_seqlen, src_word2id)\n",
    "enc_src_dev = encode_sentences(src_dev, maxsrc_seqlen, src_word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_trg_train = encode_sentences(trg_train, maxtrg_seqlen, trg_word2id)\n",
    "enc_trg_test = encode_sentences(trg_test, maxtrg_seqlen, trg_word2id)\n",
    "enc_trg_dev = encode_sentences(trg_dev, maxtrg_seqlen, trg_word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode_sentences(enc_src_train[4] ,src_id2word)\n",
    "# decode_sentences(enc_trg_train[4] ,trg_id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "def create_data_loader(data):\n",
    "    data = torch.tensor(data)\n",
    "    data = TensorDataset(data)\n",
    "    data_dataloader = DataLoader(data, batch_size = BATCH_SIZE)\n",
    "    \n",
    "    return data_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_src_train_dl = create_data_loader(enc_src_train)\n",
    "enc_src_test_dl = create_data_loader(enc_src_test)\n",
    "enc_src_dev_dl = create_data_loader(enc_src_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_trg_train_dl = create_data_loader(enc_trg_train)\n",
    "enc_trg_test_dl = create_data_loader(enc_trg_test)\n",
    "enc_trg_dev_dl = create_data_loader(enc_trg_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(src_word2id)\n",
    "OUTPUT_DIM = len(trg_word2id)\n",
    "ENC_EMB_DIM = 250\n",
    "DEC_EMB_DIM = 250\n",
    "HID_DIM = 256\n",
    "NUM_LAYERS = 1\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "N_EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, num_layers, p):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc_dropout = nn.Dropout(p)\n",
    "        self.enc_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.enc_LSTM = nn.LSTM(emb_dim, hid_dim, num_layers, dropout = p)\n",
    "        \n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.uniform_(self.enc_embedding.weight, -0.08, 0.08)\n",
    "        nn.init.uniform_(self.enc_LSTM.weight_ih_l0, -0.08, 0.08)\n",
    "        nn.init.uniform_(self.enc_LSTM.weight_hh_l0, -0.08, 0.08)\n",
    "        nn.init.uniform_(self.enc_LSTM.bias_ih_l0, -0.08, 0.08)\n",
    "        nn.init.uniform_(self.enc_LSTM.bias_hh_l0, -0.08, 0.08)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        embedded = self.enc_dropout(self.enc_embedding(src))\n",
    "        o, (h, c) = self.enc_LSTM(embedded)\n",
    "        \n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, num_layers, p):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dec_dropout = nn.Dropout(p)\n",
    "        self.dec_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.dec_LSTM = nn.LSTM(emb_dim, hid_dim, num_layers, dropout = p)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \n",
    "        nn.init.uniform_(self.dec_embedding.weight, -0.08, 0.08)\n",
    "        nn.init.uniform_(self.dec_LSTM.weight_ih_l0, -0.08, 0.08)\n",
    "        nn.init.uniform_(self.dec_LSTM.weight_hh_l0, -0.08, 0.08)\n",
    "        nn.init.uniform_(self.dec_LSTM.bias_ih_l0, -0.08, 0.08)\n",
    "        nn.init.uniform_(self.dec_LSTM.bias_hh_l0, -0.08, 0.08)\n",
    "        nn.init.uniform_(self.fc_out.weight, -0.08, 0.08)\n",
    "        nn.init.uniform_(self.fc_out.bias, -0.08, 0.08)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        embedded = self.dec_dropout(self.dec_embedding(input))\n",
    "        o, (h, c) = self.dec_LSTM(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(o.squeeze(0))\n",
    "        \n",
    "        return prediction, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src_batch, trg_batch, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        trg_len,batch_size = trg_batch.shape\n",
    "        trg_vocab_size = len(trg_id2word)\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        hidden, cell = self.encoder(src_batch)\n",
    "        \n",
    "        input = trg_batch[0,:]\n",
    "        \n",
    "        answer = list()\n",
    "        \n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input.unsqueeze(0), hidden, cell)\n",
    "            outputs[t] = output\n",
    "            answer.append(output.argmax(1).tolist())\n",
    "\n",
    "            if(random.random() < teacher_forcing_ratio):\n",
    "                input = trg_batch[t]\n",
    "            else:\n",
    "                input = output.argmax(1)       \n",
    "        \n",
    "        return outputs , answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, NUM_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, NUM_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "''' Ignoring Padding index '''\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, src_iterator, trg_iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (batch_src, batch_trg) in enumerate(zip(src_iterator, trg_iterator)):\n",
    "        \n",
    "        b_src = batch_src[0]\n",
    "        b_src = b_src.to(device)\n",
    "        src_batch = torch.transpose(b_src,0,1)\n",
    "        \n",
    "        b_trg = batch_trg[0]\n",
    "        b_trg = b_trg.to(device)\n",
    "        trg_batch = torch.transpose(b_trg,0,1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, answer = model(src_batch, trg_batch)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        \n",
    "        trg_batch = trg_batch.contiguous() \n",
    "        trg_batch = trg_batch[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(src_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, src_iterator, trg_iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    targets = []\n",
    "    answers = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, (batch_src, batch_trg) in enumerate(zip(src_iterator, trg_iterator)):\n",
    "        \n",
    "            b_src = batch_src[0]\n",
    "            b_src = b_src.to(device)\n",
    "            src_batch = torch.transpose(b_src,0,1)\n",
    "\n",
    "            b_trg = batch_trg[0]\n",
    "            b_trg = b_trg.to(device)\n",
    "            trg_batch = torch.transpose(b_trg,0,1)\n",
    "\n",
    "            targets.append(trg_batch.tolist())\n",
    "            output, answer = model(src_batch, trg_batch)\n",
    "            answers.append(answer)\n",
    "            \n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "\n",
    "            trg_batch = trg_batch.contiguous()\n",
    "            trg_batch = trg_batch[1:].view(-1)\n",
    "\n",
    "\n",
    "            loss = criterion(output, trg_batch)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(src_iterator), targets, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss = train(model, enc_src_train_dl, enc_trg_train_dl, optimizer, criterion, 1)\n",
    "    valid_loss, targets, answers = evaluate(model, enc_src_dev_dl, enc_trg_dev_dl , criterion)\n",
    "    \n",
    "    val_losses.append(valid_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        torch.save(model.state_dict(), 'best-model-1.pt')\n",
    "        best_valid_loss = valid_loss\n",
    "        \n",
    "    \n",
    "    print('Epoch no.:', epoch+1)\n",
    "    print('Train Loss:',round(train_loss,3))\n",
    "    print('Val. Loss:', round(valid_loss,3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(l1): \n",
    "      \n",
    "    l2 = list()    \n",
    "    for i in range(len(l1[0])): \n",
    "        row =[] \n",
    "        for item in l1: \n",
    "            row.append(item[i]) \n",
    "        l2.append(row) \n",
    "    return l2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_blue_scores(targets, predicted):\n",
    "    candidate_corpus = list()\n",
    "    reference_corpus = list()\n",
    "    \n",
    "    for batch in predicted:\n",
    "        batch = transpose(batch)\n",
    "        for encoded_sentence in batch:\n",
    "            decoded_sentence = decode_sentences(encoded_sentence ,trg_id2word)\n",
    "            candidate_corpus.append(decoded_sentence)\n",
    "            \n",
    "    for batch in targets:\n",
    "        batch = transpose(batch)\n",
    "        for encoded_sentence in batch:\n",
    "            decoded_sentence = decode_sentences(encoded_sentence ,trg_id2word)\n",
    "            reference_corpus.append(decoded_sentence)\n",
    "        \n",
    "    \n",
    "    print(\"BLUE@1 Score:\", round(bleu_score(candidate_corpus, reference_corpus, max_n=1, weights=[1]),3))\n",
    "    print(\"BLUE@2 Score:\", round(bleu_score(candidate_corpus, reference_corpus, max_n=2, weights=[0.5, 0.5]),3))\n",
    "    print(\"BLUE@3 Score:\", round(bleu_score(candidate_corpus, reference_corpus, max_n=3, weights=[0.33, 0.33, 0.33]),3))\n",
    "    print(\"BLUE@4 Score:\", round(bleu_score(candidate_corpus, reference_corpus, max_n=4, weights=[0.25, 0.25, 0.25, 0.25]),3))    \n",
    "    \n",
    "    return candidate_corpus , reference_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graphs():\n",
    "    \n",
    "    epoch_no = []\n",
    "    for i in range(len(train_losses)):\n",
    "        epoch_no.append(i + 1)\n",
    "    plt.plot(epoch_no, train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translated_sentences(candidate_corpus , reference_corpus):\n",
    "    f = open(\"translated_sent1.txt\", \"w\")\n",
    "    \n",
    "    for i in range(len(candidate_corpus)):\n",
    "        print(\"Predicted: \", \" \".join(word for word in candidate_corpus[i]),\", Actual:\",\" \".join(word for word in reference_corpus[i]))\n",
    "        \n",
    "        f.write(\"Predicted: \" + \" \".join(word for word in candidate_corpus[i]) + \"    Actual: \" + \" \".join(word for word in reference_corpus[i]) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best-model-1.pt'))\n",
    "\n",
    "test_loss, targets, predicted = evaluate(model, enc_src_test_dl, enc_trg_test_dl, criterion)\n",
    "\n",
    "print('Test Loss:',round(test_loss,3))\n",
    "\n",
    "candidate_corpus , reference_corpus = calculate_blue_scores(targets, predicted)\n",
    "generate_graphs()\n",
    "print_translated_sentences(candidate_corpus , reference_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
